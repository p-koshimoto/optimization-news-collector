import arxiv
import feedparser
import requests
from datetime import datetime, timedelta
import json
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.base import MIMEBase
from email import encoders
import os
import sys
import pytz
import time

class OptimizationNewsCollector:
    def __init__(self):
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
        self.recipient_email = os.getenv('RECIPIENT_EMAIL')
        self.sender_email = os.getenv('SENDER_EMAIL')
        self.sender_password = os.getenv('GMAIL_APP_PASSWORD')
        
        # æ—¥æœ¬æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®š
        self.jst = pytz.timezone('Asia/Tokyo')
        
    def get_jst_time(self):
        """ç¾åœ¨ã®æ—¥æœ¬æ™‚é–“ã‚’å–å¾—"""
        return datetime.now(self.jst)


    def simple_arxiv_test(self):
        """æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªarXivãƒ†ã‚¹ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        print("æœ€æ–°5ä»¶ã® math.OC è«–æ–‡:")
        
        try:
            # Method 1: arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼‰
            client = arxiv.Client()
            
            # ã‚ˆã‚Šå®‰å…¨ãªè¨­å®š
            search = arxiv.Search(
                query="cat:math.OC",
                max_results=5,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            
            # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§å®Ÿè¡Œ
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                try:
                    results = list(client.results(search))
                    break
                except (arxiv.arxiv.HTTPError, requests.exceptions.RequestException) as e:
                    retry_count += 1
                    print(f"  âš ï¸ è©¦è¡Œ {retry_count}/{max_retries} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    if retry_count < max_retries:
                        print(f"  â³ {2 ** retry_count}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                        time.sleep(2 ** retry_count)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                    else:
                        print("  âŒ arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä»£æ›¿æ–¹æ³•ã‚’è©¦ã—ã¾ã™...")
                        return self.fallback_arxiv_test()
            
            for i, result in enumerate(results, 1):
                print(f"{i}. {result.title}")
                print(f"   Published: {result.published}")
                print(f"   URL: {result.entry_id}")
                print()
                
            print(f"âœ… arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ {len(results)} ä»¶å–å¾—æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚¨ãƒ©ãƒ¼: {e}")
            print("ä»£æ›¿æ–¹æ³•ã‚’è©¦ã—ã¾ã™...")
            return self.fallback_arxiv_test()
    
    def fallback_arxiv_test(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç›´æ¥APIã‚’å©ãæ–¹æ³•"""
        print("\n--- ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´æ¥APIå‘¼ã³å‡ºã— ---")
        
        try:
            # arXiv APIã‚’ç›´æ¥å‘¼ã³å‡ºã—
            api_url = "https://export.arxiv.org/api/query"
            params = {
                'search_query': 'cat:math.OC',
                'start': 0,
                'max_results': 5,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            
            response = requests.get(api_url, params=params, timeout=30)
            response.raise_for_status()
            
            # feedparserã§XMLã‚’è§£æ
            feed = feedparser.parse(response.content)
            
            if not feed.entries:
                print("âŒ APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return False
            
            print("æœ€æ–°5ä»¶ã® math.OC è«–æ–‡ (ç›´æ¥API):")
            for i, entry in enumerate(feed.entries, 1):
                print(f"{i}. {entry.title}")
                print(f"   Published: {entry.published}")
                print(f"   URL: {entry.id}")
                print()
            
            print(f"âœ… ç›´æ¥APIå‘¼ã³å‡ºã—ã§ {len(feed.entries)} ä»¶å–å¾—æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ ç›´æ¥APIå‘¼ã³å‡ºã—ã§ã‚‚ã‚¨ãƒ©ãƒ¼: {e}")
            return False




    
    def collect_arxiv_papers_fixed(self, days_back=2):
        """ä¿®æ­£ç‰ˆï¼šarXivã‹ã‚‰æ•°ç†æœ€é©åŒ–é–¢é€£è«–æ–‡ã‚’åé›†"""
        print("ğŸ“š arXivã‹ã‚‰è«–æ–‡ã‚’åé›†ä¸­...")
        
        papers = []
        cutoff_date = self.get_jst_time().date() - timedelta(days=days_back)
        
        # Method 1: arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’è©¦ã™
        try:
            client = arxiv.Client()
            search = arxiv.Search(
                query=(
                    "cat:math.OC OR "
                    "(cat:cs.DM AND (optimization OR programming OR algorithm)) OR "
                    "(cat:stat.ML AND optimization) OR "
                    'ti:"linear programming" OR ti:"integer programming" OR '
                    'ti:"convex optimization" OR ti:"nonlinear programming" OR '
                    'ti:"combinatorial optimization" OR ti:"stochastic optimization"'
                ),
                max_results=50,
                sort_by=arxiv.SortCriterion.SubmittedDate,
                sort_order=arxiv.SortOrder.Descending
            )
            
            # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§å®Ÿè¡Œ
            results = []
            retry_count = 0
            max_retries = 3
            
            while retry_count < max_retries:
                try:
                    results = list(client.results(search))
                    break
                except Exception as e:
                    retry_count += 1
                    print(f"  âš ï¸ arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªè©¦è¡Œ {retry_count}/{max_retries} ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    if retry_count < max_retries:
                        time.sleep(2 ** retry_count)
                    else:
                        print("  âŒ arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«å¤±æ•—ã€ç›´æ¥APIå‘¼ã³å‡ºã—ã‚’è©¦ã—ã¾ã™...")
                        return self.collect_arxiv_papers_direct_api(days_back)
            
            # çµæœã‚’å‡¦ç†
            for result in results:
                published_jst = result.published.astimezone(self.jst).date()
                updated_jst = result.updated.astimezone(self.jst).date() if result.updated else None
                
                if published_jst >= cutoff_date or (updated_jst and updated_jst >= cutoff_date):
                    papers.append({
                        'title': result.title.replace('\n', ' ').strip(),
                        'authors': [author.name for author in result.authors[:3]],
                        'abstract': result.summary.replace('\n', ' ').strip()[:500] + "...",
                        'url': result.entry_id,
                        'published': published_jst.strftime('%Y-%m-%d'),
                        'updated': updated_jst.strftime('%Y-%m-%d') if updated_jst else None,
                        'categories': result.categories
                    })
            
            print(f"âœ… arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§è«–æ–‡ {len(papers)} ä»¶ã‚’åé›†ã—ã¾ã—ãŸ")
            return papers
            
        except Exception as e:
            print(f"âŒ arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚¨ãƒ©ãƒ¼: {e}")
            print("ç›´æ¥APIå‘¼ã³å‡ºã—ã‚’è©¦ã—ã¾ã™...")
            return self.collect_arxiv_papers_direct_api(days_back)

    
    def collect_arxiv_papers_direct_api(self, days_back=2):
        """ç›´æ¥APIå‘¼ã³å‡ºã—ã§arXivè«–æ–‡ã‚’åé›†"""
        print("ğŸ“š ç›´æ¥APIã§arXivã‹ã‚‰è«–æ–‡ã‚’åé›†ä¸­...")
        
        papers = []
        cutoff_date = self.get_jst_time().date() - timedelta(days=days_back)
        
        try:
            # ã‚¯ã‚¨ãƒªã‚’åˆ†å‰²ã—ã¦ãã‚Œãã‚Œå–å¾—
            queries = [
                "cat:math.OC",
                "cat:cs.DM AND optimization",
                "cat:stat.ML AND optimization"
            ]
            
            for query in queries:
                try:
                    api_url = "https://export.arxiv.org/api/query"
                    params = {
                        'search_query': query,
                        'start': 0,
                        'max_results': 20,
                        'sortBy': 'submittedDate',
                        'sortOrder': 'descending'
                    }
                    
                    response = requests.get(api_url, params=params, timeout=30)
                    response.raise_for_status()
                    
                    feed = feedparser.parse(response.content)
                    
                    for entry in feed.entries:
                        # æ—¥ä»˜å‡¦ç†
                        try:
                            published_dt = datetime.strptime(entry.published, '%Y-%m-%dT%H:%M:%SZ')
                            published_dt = pytz.utc.localize(published_dt)
                            published_jst = published_dt.astimezone(self.jst).date()
                            
                            updated_jst = None
                            if hasattr(entry, 'updated'):
                                try:
                                    updated_dt = datetime.strptime(entry.updated, '%Y-%m-%dT%H:%M:%SZ')
                                    updated_dt = pytz.utc.localize(updated_dt)
                                    updated_jst = updated_dt.astimezone(self.jst).date()
                                except:
                                    pass
                            
                            # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿
                            if published_jst >= cutoff_date or (updated_jst and updated_jst >= cutoff_date):
                                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                                if not any(p['url'] == entry.id for p in papers):
                                    # è‘—è€…å‡¦ç†
                                    authors = []
                                    if hasattr(entry, 'authors'):
                                        authors = [author.name for author in entry.authors[:3]]
                                    elif hasattr(entry, 'author'):
                                        authors = [entry.author]
                                    
                                    # ã‚«ãƒ†ã‚´ãƒªå‡¦ç†
                                    categories = []
                                    if hasattr(entry, 'arxiv_primary_category'):
                                        categories.append(entry.arxiv_primary_category['term'])
                                    
                                    papers.append({
                                        'title': entry.title.replace('\n', ' ').strip(),
                                        'authors': authors,
                                        'abstract': entry.summary.replace('\n', ' ').strip()[:500] + "...",
                                        'url': entry.id,
                                        'published': published_jst.strftime('%Y-%m-%d'),
                                        'updated': updated_jst.strftime('%Y-%m-%d') if updated_jst else None,
                                        'categories': categories
                                    })
                        
                        except Exception as e:
                            print(f"  âš ï¸ ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                
                except Exception as e:
                    print(f"  âš ï¸ ã‚¯ã‚¨ãƒª '{query}' ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
                
                # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
                time.sleep(1)
            
            print(f"âœ… ç›´æ¥APIã§è«–æ–‡ {len(papers)} ä»¶ã‚’åé›†ã—ã¾ã—ãŸ")
            return papers
            
        except Exception as e:
            print(f"âŒ ç›´æ¥APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    
    def collect_news_from_rss(self):
        """RSSã‹ã‚‰æœ€é©åŒ–é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰"""
        print("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...")
        
        # æ•°ç†æœ€é©åŒ–ã«ã‚ˆã‚Šé–¢é€£æ€§ã®é«˜ã„RSSã‚½ãƒ¼ã‚¹
        rss_urls = [
            "https://rss.cnn.com/rss/edition_technology.rss",
            "https://feeds.reuters.com/reuters/technologyNews",
            "https://rss.slashdot.org/Slashdot/slashdotMain",
            "https://feeds.feedburner.com/oreilly/radar"
        ]
        
        # ã‚ˆã‚Šå³å¯†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
        optimization_keywords = [
            'optimization', 'optimisation', 'algorithm', 'programming',
            'linear programming', 'integer programming', 'convex',
            'machine learning', 'data science', 'operations research',
            'mathematical programming', 'solver', 'constraint',
            'heuristic', 'metaheuristic', 'genetic algorithm',
            'simulated annealing', 'particle swarm', 'gradient descent',
            'neural network', 'deep learning', 'reinforcement learning'
        ]
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé–¢é€£æ€§ã®ä½ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é™¤å¤–ï¼‰
        exclude_keywords = [
            'celebrity', 'entertainment', 'sports', 'weather',
            'politics', 'election', 'crime', 'accident', 'war',
            'fashion', 'food', 'travel', 'celebrity', 'gossip'
        ]
        
        news_items = []
        for rss_url in rss_urls:
            try:
                feed = feedparser.parse(rss_url)
                for entry in feed.entries[:10]:  # å„RSSã‹ã‚‰10ä»¶ãƒã‚§ãƒƒã‚¯
                    title_lower = entry.title.lower()
                    summary_lower = getattr(entry, 'summary', '').lower()
                    combined_text = title_lower + ' ' + summary_lower
                    
                    # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                    if any(exclude in combined_text for exclude in exclude_keywords):
                        continue
                    
                    # æœ€é©åŒ–é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
                    relevance_score = sum(1 for keyword in optimization_keywords 
                                        if keyword in combined_text)
                    
                    # é–¢é€£åº¦ã‚¹ã‚³ã‚¢ãŒ2ä»¥ä¸Šã®è¨˜äº‹ã®ã¿æ¡ç”¨
                    if relevance_score >= 1:
                        # æ—¥æœ¬æ™‚é–“ã§å…¬é–‹æ—¥ã‚’å‡¦ç†
                        published_date = getattr(entry, 'published', '')
                        if published_date:
                            try:
                                pub_dt = datetime.strptime(published_date[:19], '%Y-%m-%dT%H:%M:%S')
                                pub_dt = pytz.utc.localize(pub_dt).astimezone(self.jst)
                                published_jst = pub_dt.strftime('%Y-%m-%d %H:%M JST')
                            except:
                                published_jst = published_date
                        else:
                            published_jst = 'æ—¥æ™‚ä¸æ˜'
                        
                        news_items.append({
                            'title': entry.title,
                            'link': entry.link,
                            'published': published_jst,
                            'summary': getattr(entry, 'summary', '')[:300] + "...",
                            'relevance_score': relevance_score
                        })
                        
                        # ååˆ†ãªæ•°ã®é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒé›†ã¾ã£ãŸã‚‰çµ‚äº†
                        if len(news_items) >= 8:
                            break
                            
            except Exception as e:
                print(f"âš ï¸ RSSå–å¾—ã‚¨ãƒ©ãƒ¼ ({rss_url}): {e}")
                continue
        
        # é–¢é€£åº¦ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        news_items.sort(key=lambda x: x['relevance_score'], reverse=True)
        news_items = news_items[:5]  # ä¸Šä½5ä»¶ã®ã¿
        
        print(f"âœ… é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(news_items)} ä»¶ã‚’åé›†ã—ã¾ã—ãŸ")
        return news_items

    def collect_news_from_rss_improved(self):
        """æ”¹å–„ç‰ˆï¼šRSSã‹ã‚‰æœ€é©åŒ–é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        print("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ä¸­...")
        
        # ã‚ˆã‚Šç¾åœ¨ã§ã‚‚ä½¿ãˆã‚‹RSSã‚½ãƒ¼ã‚¹ï¼ˆ2024å¹´å¯¾å¿œï¼‰
        rss_urls = [
            # ã‚ˆã‚Šç¢ºå®Ÿã«å‹•ä½œã™ã‚‹RSSãƒ•ã‚£ãƒ¼ãƒ‰
            "https://www.theverge.com/rss/index.xml",
            "https://techcrunch.com/feed/",
            "https://www.wired.com/feed/rss",
            "https://arstechnica.com/feed/",
            "https://feeds.feedburner.com/venturebeat/SZYF",
            "https://www.zdnet.com/news/rss.xml",
            "https://rss.cnn.com/rss/edition_technology.rss",  # å¿µã®ãŸã‚æ®‹ã™
            "https://feeds.reuters.com/reuters/technologyNews"  # å¿µã®ãŸã‚æ®‹ã™
        ]
        
        # ã‚ˆã‚ŠæŸ”è»Ÿãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆæ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
        # Tier 1: ç›´æ¥é–¢é€£ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
        high_priority_keywords = [
            'optimization', 'optimisation', 'algorithm', 'programming',
            'machine learning', 'AI', 'artificial intelligence',
            'data science', 'operations research', 'solver'
        ]
        
        # Tier 2: é–“æ¥é–¢é€£ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰
        medium_priority_keywords = [
            'analytics', 'efficiency', 'performance', 'automation',
            'neural network', 'deep learning', 'model', 'prediction',
            'computational', 'mathematical', 'statistical'
        ]
        
        # Tier 3: æŠ€è¡“é–¢é€£ï¼ˆä½ã‚¹ã‚³ã‚¢ï¼‰
        low_priority_keywords = [
            'software', 'technology', 'tech', 'innovation',
            'research', 'development', 'computing', 'digital'
        ]
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¸›ã‚‰ã™ï¼ˆéåº¦ãªé™¤å¤–ã‚’é˜²ãï¼‰
        exclude_keywords = [
            'celebrity', 'entertainment', 'sports', 'weather',
            'crime', 'accident', 'war', 'fashion', 'food', 'travel'
        ]
        
        news_items = []
        
        for rss_url in rss_urls:
            try:
                print(f"  ğŸ” å–å¾—ä¸­: {rss_url}")
                
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                # ã¾ãšHTTPã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                try:
                    response = requests.get(rss_url, headers=headers, timeout=10)
                    if response.status_code != 200:
                        print(f"    âš ï¸ HTTP {response.status_code}: {rss_url}")
                        continue
                except Exception as e:
                    print(f"    âš ï¸ ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
                
                # feedparserã§è§£æ
                feed = feedparser.parse(rss_url)
                
                if not feed.entries:
                    print(f"    âš ï¸ ã‚¨ãƒ³ãƒˆãƒªãªã—: {rss_url}")
                    continue
                
                print(f"    âœ… {len(feed.entries)}ä»¶ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—")
                
                # ã‚ˆã‚Šå¤šãã®ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ20ä»¶ã«å¢—åŠ ï¼‰
                for entry in feed.entries[:20]:
                    try:
                        title_lower = entry.title.lower()
                        summary_lower = getattr(entry, 'summary', '').lower()
                        combined_text = title_lower + ' ' + summary_lower
                        
                        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆç·©å’Œï¼‰
                        exclude_count = sum(1 for exclude in exclude_keywords 
                                          if exclude in combined_text)
                        if exclude_count >= 2:  # 2å€‹ä»¥ä¸Šã®é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã®ã¿é™¤å¤–
                            continue
                        
                        # æ®µéšçš„é–¢é€£åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
                        high_score = sum(2 for keyword in high_priority_keywords 
                                       if keyword in combined_text)
                        medium_score = sum(1 for keyword in medium_priority_keywords 
                                         if keyword in combined_text)
                        low_score = sum(0.5 for keyword in low_priority_keywords 
                                      if keyword in combined_text)
                        
                        total_relevance_score = high_score + medium_score + low_score
                        
                        # ã‚ˆã‚Šç·©ã„é–¾å€¤ï¼ˆ1.0ä»¥ä¸Šã§æ¡ç”¨ï¼‰
                        if total_relevance_score >= 1.0:
                            # æ—¥æœ¬æ™‚é–“ã§å…¬é–‹æ—¥ã‚’å‡¦ç†
                            published_date = getattr(entry, 'published', '')
                            if published_date:
                                try:
                                    # ã‚ˆã‚ŠæŸ”è»Ÿãªæ—¥ä»˜ãƒ‘ãƒ¼ã‚¹
                                    from dateutil import parser
                                    pub_dt = parser.parse(published_date)
                                    if pub_dt.tzinfo is None:
                                        pub_dt = pytz.utc.localize(pub_dt)
                                    published_jst = pub_dt.astimezone(self.jst).strftime('%Y-%m-%d %H:%M JST')
                                except:
                                    published_jst = published_date[:19] if len(published_date) > 19 else published_date
                            else:
                                published_jst = 'æ—¥æ™‚ä¸æ˜'
                            
                            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆURLãƒ™ãƒ¼ã‚¹ï¼‰
                            if not any(item['link'] == entry.link for item in news_items):
                                news_items.append({
                                    'title': entry.title.strip(),
                                    'link': entry.link,
                                    'published': published_jst,
                                    'summary': getattr(entry, 'summary', '')[:300] + "...",
                                    'relevance_score': round(total_relevance_score, 1),
                                    'source_url': rss_url  # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ã‚½ãƒ¼ã‚¹ã‚’è¨˜éŒ²
                                })
                                
                                print(f"    ğŸ“„ æ¡ç”¨: {entry.title[:50]}... (ã‚¹ã‚³ã‚¢: {total_relevance_score:.1f})")
                    
                    except Exception as e:
                        print(f"    âš ï¸ ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
                        
            except Exception as e:
                print(f"  âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼ ({rss_url}): {e}")
                continue
            
            # APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
            time.sleep(0.5)
        
        # é–¢é€£åº¦ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        news_items.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # ä¸Šä½10ä»¶ã«åˆ¶é™ï¼ˆå…ƒã®5ä»¶ã‹ã‚‰å¢—åŠ ï¼‰
        news_items = news_items[:10]
        
        print(f"âœ… é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(news_items)} ä»¶ã‚’åé›†ã—ã¾ã—ãŸ")
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›
        if news_items:
            print("ğŸ“Š åé›†ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è©³ç´°:")
            for i, item in enumerate(news_items, 1):
                print(f"  {i}. ã‚¹ã‚³ã‚¢{item['relevance_score']}: {item['title'][:60]}...")
        else:
            print("âš ï¸ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒ0ä»¶ã§ã™ã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š")
            print("  1. RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ã‚¢ã‚¯ã‚»ã‚¹çŠ¶æ³")
            print("  2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ã®è¨­å®š")
            print("  3. é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¨­å®š")
        
        return news_items
    
    def generate_html_report(self, papers, news_items):
        """ç¾ã—ã„HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        jst_now = self.get_jst_time()
        
        # HTMLãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        html_report = f"""
        <!DOCTYPE html>
        <html lang="ja">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>æ•°ç†æœ€é©åŒ– æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆ</title>
            <style>
                body {{
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    line-height: 1.6;
                    margin: 0;
                    padding: 20px;
                    background-color: #f5f5f5;
                    color: #333;
                }}
                .container {{
                    max-width: 800px;
                    margin: 0 auto;
                    background-color: white;
                    border-radius: 10px;
                    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
                    overflow: hidden;
                }}
                .header {{
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    color: white;
                    padding: 30px;
                    text-align: center;
                }}
                .header h1 {{
                    margin: 0;
                    font-size: 28px;
                    font-weight: 300;
                }}
                .header .date {{
                    margin-top: 10px;
                    font-size: 16px;
                    opacity: 0.9;
                }}
                .section {{
                    margin: 20px;
                }}
                .section-title {{
                    font-size: 22px;
                    font-weight: 600;
                    margin: 30px 0 20px 0;
                    padding: 15px;
                    border-radius: 8px;
                    display: flex;
                    align-items: center;
                }}
                .section-title.papers {{
                    background-color: #e3f2fd;
                    border-left: 5px solid #2196f3;
                    color: #1976d2;
                }}
                .section-title.news {{
                    background-color: #fff8e1;
                    border-left: 5px solid #ff9800;
                    color: #f57c00;
                }}
                .item {{
                    background-color: white;
                    border: 1px solid #e0e0e0;
                    border-radius: 8px;
                    margin: 15px 0;
                    padding: 20px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.05);
                    transition: box-shadow 0.3s ease;
                }}
                .item:hover {{
                    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                }}
                .item-title {{
                    font-size: 18px;
                    font-weight: 600;
                    margin-bottom: 12px;
                    color: #2c3e50;
                    line-height: 1.4;
                }}
                .item-meta {{
                    display: flex;
                    flex-wrap: wrap;
                    gap: 15px;
                    margin-bottom: 12px;
                    font-size: 14px;
                    color: #666;
                }}
                .meta-item {{
                    display: flex;
                    align-items: center;
                }}
                .meta-label {{
                    font-weight: 600;
                    margin-right: 5px;
                }}
                .abstract {{
                    color: #555;
                    line-height: 1.6;
                    margin-bottom: 15px;
                }}
                .link {{
                    display: inline-block;
                    background-color: #4CAF50;
                    color: white;
                    padding: 8px 16px;
                    text-decoration: none;
                    border-radius: 4px;
                    font-size: 14px;
                    transition: background-color 0.3s ease;
                }}
                .link:hover {{
                    background-color: #45a049;
                }}
                .news-link {{
                    background-color: #ff9800;
                }}
                .news-link:hover {{
                    background-color: #f57c00;
                }}
                .relevance-stars {{
                    color: #ffc107;
                    font-size: 16px;
                }}
                .stats {{
                    background-color: #f8f9fa;
                    border-radius: 8px;
                    padding: 20px;
                    margin: 30px 20px;
                    text-align: center;
                }}
                .stats-grid {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
                    gap: 20px;
                    margin-top: 15px;
                }}
                .stat-item {{
                    text-align: center;
                }}
                .stat-number {{
                    font-size: 24px;
                    font-weight: 700;
                    color: #2196f3;
                }}
                .stat-label {{
                    font-size: 14px;
                    color: #666;
                    margin-top: 5px;
                }}
                .footer {{
                    text-align: center;
                    padding: 20px;
                    color: #999;
                    font-size: 12px;
                    border-top: 1px solid #eee;
                }}
                .no-content {{
                    text-align: center;
                    padding: 40px;
                    color: #999;
                    font-style: italic;
                }}
                .emoji {{
                    margin-right: 8px;
                }}
                @media (max-width: 600px) {{
                    .container {{
                        margin: 10px;
                        border-radius: 5px;
                    }}
                    .header {{
                        padding: 20px;
                    }}
                    .header h1 {{
                        font-size: 24px;
                    }}
                    .section {{
                        margin: 15px;
                    }}
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>ğŸ”¬ æ•°ç†æœ€é©åŒ– æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆ</h1>
                    <div class="date">{jst_now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} JST</div>
                </div>
                
                <div class="section">
                    <div class="section-title papers">
                        <span class="emoji">ğŸ“š</span>
                        æ–°ç€è«–æ–‡ ({len(papers)}ä»¶)
                    </div>
        """
        
        # è«–æ–‡ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if papers:
            for i, paper in enumerate(papers, 1):
                authors_str = ', '.join(paper['authors'])
                if len(paper['authors']) > 3:
                    authors_str += " ä»–"
                
                categories_str = ', '.join(paper['categories'][:2])
                
                html_report += f"""
                    <div class="item">
                        <div class="item-title">{i}. {paper['title']}</div>
                        <div class="item-meta">
                            <div class="meta-item">
                                <span class="meta-label">ğŸ‘¥ è‘—è€…:</span>
                                {authors_str}
                            </div>
                            <div class="meta-item">
                                <span class="meta-label">ğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒª:</span>
                                {categories_str}
                            </div>
                            <div class="meta-item">
                                <span class="meta-label">ğŸ“… å…¬é–‹æ—¥:</span>
                                {paper['published']}
                            </div>
                        </div>
                        <div class="abstract">{paper['abstract']}</div>
                        <a href="{paper['url']}" class="link" target="_blank">è«–æ–‡ã‚’èª­ã‚€</a>
                    </div>
                """
        else:
            html_report += '<div class="no-content">æœ¬æ—¥ã¯æ–°ç€è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</div>'
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        html_report += f"""
                </div>
                
                <div class="section">
                    <div class="section-title news">
                        <span class="emoji">ğŸ“°</span>
                        æ•°ç†æœ€é©åŒ–é–¢é€£æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ ({len(news_items)}ä»¶)
                    </div>
        """
        
        if news_items:
            for i, news in enumerate(news_items, 1):
                stars = 'â­' * news['relevance_score']
                
                html_report += f"""
                    <div class="item">
                        <div class="item-title">{i}. {news['title']}</div>
                        <div class="item-meta">
                            <div class="meta-item">
                                <span class="meta-label">ğŸ¯ é–¢é€£åº¦:</span>
                                <span class="relevance-stars">{stars}</span>
                            </div>
                            <div class="meta-item">
                                <span class="meta-label">ğŸ“… å…¬é–‹æ—¥:</span>
                                {news['published']}
                            </div>
                        </div>
                        <div class="abstract">{news['summary']}</div>
                        <a href="{news['link']}" class="link news-link" target="_blank">è¨˜äº‹ã‚’èª­ã‚€</a>
                    </div>
                """
        else:
            html_report += '<div class="no-content">æœ¬æ—¥ã¯é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚</div>'
        
        # çµ±è¨ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
        html_report += f"""
                </div>
                
                <div class="stats">
                    <h3>ğŸ“Š åé›†çµ±è¨ˆ</h3>
                    <div class="stats-grid">
                        <div class="stat-item">
                            <div class="stat-number">{len(papers)}</div>
                            <div class="stat-label">è«–æ–‡æ•°</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-number">{len(news_items)}</div>
                            <div class="stat-label">ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-number">{jst_now.strftime('%H:%M')}</div>
                            <div class="stat-label">ç”Ÿæˆæ™‚åˆ» (JST)</div>
                        </div>
                    </div>
                </div>
                
                <div class="footer">
                    ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ<br>
                    æ—¥æœ¬æ¨™æº–æ™‚ (JST) - {jst_now.strftime('%Y-%m-%d %H:%M:%S')}
                </div>
            </div>
        </body>
        </html>
        """
        
        return html_report
    
    def generate_text_report(self, papers, news_items):
        """ãƒ†ã‚­ã‚¹ãƒˆç‰ˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆDiscordç”¨ãªã©ï¼‰"""
        jst_now = self.get_jst_time()
        
        report = f"""
# ğŸ”¬ æ•°ç†æœ€é©åŒ– æ—¥æ¬¡ãƒ¬ãƒãƒ¼ãƒˆ
**ç”Ÿæˆæ—¥æ™‚**: {jst_now.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')} JST

---

## ğŸ“š æ–°ç€è«–æ–‡ ({len(papers)}ä»¶)

"""
        
        if papers:
            for i, paper in enumerate(papers, 1):
                authors_str = ', '.join(paper['authors'])
                if len(paper['authors']) > 3:
                    authors_str += " ä»–"
                
                report += f"""
### {i}. {paper['title']}

- **è‘—è€…**: {authors_str}
- **ã‚«ãƒ†ã‚´ãƒª**: {', '.join(paper['categories'][:2])}
- **å…¬é–‹æ—¥**: {paper['published']}
- **æ¦‚è¦**: {paper['abstract']}
- **URL**: {paper['url']}

---
"""
        else:
            report += "\næœ¬æ—¥ã¯æ–°ç€è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n\n---\n"
        
        report += f"""

## ğŸ“° æ•°ç†æœ€é©åŒ–é–¢é€£æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ ({len(news_items)}ä»¶)

"""
        
        if news_items:
            for i, news in enumerate(news_items, 1):
                report += f"""
### {i}. {news['title']}

- **è¦ç´„**: {news['summary']}
- **é–¢é€£åº¦**: {'â­' * news['relevance_score']}
- **ãƒªãƒ³ã‚¯**: {news['link']}
- **å…¬é–‹æ—¥**: {news['published']}

---
"""
        else:
            report += "\næœ¬æ—¥ã¯é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n\n---\n"
        
        report += f"""

## ğŸ“Š åé›†çµ±è¨ˆ
- è«–æ–‡æ•°: {len(papers)}ä»¶
- ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {len(news_items)}ä»¶
- ç”Ÿæˆæ™‚åˆ»: {jst_now.strftime('%Y-%m-%d %H:%M:%S')} JST

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ (JST: Japan Standard Time)*
"""
        
        return report
    
    def send_email_report(self, html_report, text_report):
        """HTMLã¨ãƒ†ã‚­ã‚¹ãƒˆä¸¡æ–¹ã«å¯¾å¿œã—ãŸãƒ¡ãƒ¼ãƒ«ã‚’é€ä¿¡"""
        if not all([self.sender_email, self.sender_password, self.recipient_email]):
            print("âŒ ãƒ¡ãƒ¼ãƒ«è¨­å®šãŒä¸å®Œå…¨ã§ã™")
            return False
        
        try:
            # ãƒãƒ«ãƒãƒ‘ãƒ¼ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
            msg = MIMEMultipart('alternative')
            msg['From'] = self.sender_email
            msg['To'] = self.recipient_email
            jst_now = self.get_jst_time()
            msg['Subject'] = f"ğŸ”¬ æ•°ç†æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ - {jst_now.strftime('%Y/%m/%d')} JST"
            
            # ãƒ†ã‚­ã‚¹ãƒˆç‰ˆã¨HTMLç‰ˆã®ä¸¡æ–¹ã‚’æ·»ä»˜
            text_part = MIMEText(text_report, 'plain', 'utf-8')
            html_part = MIMEText(html_report, 'html', 'utf-8')
            
            msg.attach(text_part)
            msg.attach(html_part)
            
            # SMTPé€ä¿¡
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(self.sender_email, self.sender_password)
            text = msg.as_string()
            server.sendmail(self.sender_email, self.recipient_email, text)
            server.quit()
            
            print("âœ… HTMLãƒ¡ãƒ¼ãƒ«ã§é€ä¿¡å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def send_discord_report(self, report):
        """Discord Webhookã§ãƒ¬ãƒãƒ¼ãƒˆã‚’é€ä¿¡"""
        if not self.discord_webhook:
            print("âŒ Discord Webhook URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        try:
            # Discordã®æ–‡å­—æ•°åˆ¶é™ï¼ˆ2000æ–‡å­—ï¼‰å¯¾å¿œ
            if len(report) > 1900:
                report = report[:1900] + "\n\n[ãƒ¬ãƒãƒ¼ãƒˆãŒé•·ã„ãŸã‚åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã—ãŸ]"
            
            payload = {
                "content": f"```markdown\n{report}\n```"
            }
            
            response = requests.post(self.discord_webhook, json=payload)
            if response.status_code == 204:
                print("âœ… Discordã«é€ä¿¡å®Œäº†")
                return True
            else:
                print(f"âŒ Discordé€ä¿¡ã‚¨ãƒ©ãƒ¼: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ Discordé€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def save_report_to_file(self, html_report, text_report):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆHTMLç‰ˆã¨ãƒ†ã‚­ã‚¹ãƒˆç‰ˆä¸¡æ–¹ï¼‰"""
        jst_now = self.get_jst_time()
        timestamp = jst_now.strftime('%Y%m%d_%H%M')
        
        html_filename = f"report_{timestamp}_JST.html"
        text_filename = f"report_{timestamp}_JST.md"
        
        try:
            # HTMLç‰ˆã‚’ä¿å­˜
            with open(html_filename, 'w', encoding='utf-8') as f:
                f.write(html_report)
            print(f"âœ… HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ {html_filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
            # ãƒ†ã‚­ã‚¹ãƒˆç‰ˆã‚’ä¿å­˜
            with open(text_filename, 'w', encoding='utf-8') as f:
                f.write(text_report)
            print(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ {text_filename} ã«ä¿å­˜ã—ã¾ã—ãŸ")
            
            return html_filename, text_filename
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    
    def run_daily_collection(self):
        """æ—¥æ¬¡åé›†ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œ"""
        jst_now = self.get_jst_time()
        
        print("=" * 50)
        print(f"ğŸš€ æ—¥æ¬¡åé›†é–‹å§‹: {jst_now.strftime('%Y-%m-%d %H:%M:%S')} JST")
        print("=" * 50)

#        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
#        test_success = self.simple_arxiv_test()
#        if not test_success:
#            print("âš ï¸ ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸãŒã€æœ¬æ ¼åé›†ã‚’ç¶šè¡Œã—ã¾ã™...")

        
        # ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆä¿®æ­£ç‰ˆã‚’ä½¿ç”¨ï¼‰
        papers = self.collect_arxiv_papers_fixed(days_back=2)  # 2æ—¥åˆ†
#        news_items = self.collect_news_from_rss()
        news_items = self.collect_news_from_rss_improved()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆHTMLç‰ˆã¨ãƒ†ã‚­ã‚¹ãƒˆç‰ˆï¼‰
        html_report = self.generate_html_report(papers, news_items)
        text_report = self.generate_text_report(papers, news_items)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        self.save_report_to_file(html_report, text_report)
        
        # ãƒ¬ãƒãƒ¼ãƒˆé€ä¿¡
        email_sent = self.send_email_report(html_report, text_report)
        
        print("=" * 50)
        print("ğŸ“Š å®Ÿè¡Œçµæœ:")
        print(f"  ğŸ“š è«–æ–‡: {len(papers)}ä»¶")
        print(f"  ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹: {len(news_items)}ä»¶")
        print(f"  ğŸ“§ HTMLãƒ¡ãƒ¼ãƒ«é€ä¿¡: {'âœ…' if email_sent else 'âŒ'}")
        print(f"  ğŸ• å®Ÿè¡Œæ™‚åˆ»: {jst_now.strftime('%Y-%m-%d %H:%M:%S')} JST")
        print("=" * 50)
        
        return {
            'papers_count': len(papers),
            'news_count': len(news_items),
            'email_sent': email_sent,
            'execution_time_jst': jst_now.strftime('%Y-%m-%d %H:%M:%S JST')
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¬ æ•°ç†æœ€é©åŒ–è«–æ–‡ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    collector = OptimizationNewsCollector()
    result = collector.run_daily_collection()
    
    # çµæœã‚’JSONã§å‡ºåŠ›ï¼ˆGitHub Actionsã§ã®ç¢ºèªç”¨ï¼‰
    print("\nğŸ“„ å®Ÿè¡Œçµæœï¼ˆJSONï¼‰:")
    print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main()
